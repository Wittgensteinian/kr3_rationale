{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Wittgensteinian--KR3-9f7e41b8ab859e44\n",
      "Reusing dataset parquet (/home/gsdsaml/.cache/huggingface/datasets/parquet/Wittgensteinian--KR3-9f7e41b8ab859e44/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "kr3 = load_dataset(\"Wittgensteinian/KR3\", name='kr3', split='train')\n",
    "kr3 = kr3.remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/gsdsaml/.cache/huggingface/datasets/parquet/Wittgensteinian--KR3-9f7e41b8ab859e44/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-964db1339d22becb.arrow\n"
     ]
    }
   ],
   "source": [
    "kr3_binary = kr3.filter(lambda example: example['Rating'] != 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rating': 1,\n",
       " 'Review': '숙성 돼지고기 전문점입니다. 건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편이고 살짝 레트로 감성으로 분위기 잡아놨습니다. 모든 직원분들께서 전부 가능하다고 멘트 쳐주시며, 고기는 초반 커팅까지는 구워주십니다. 가격 저렴한 편 아니지만 맛은 준수합니다. 등심덧살이 인상 깊었는데 구이로 별로일 줄 알았는데 육향 짙고 얇게 저며 뻑뻑하지 않았습니다. 하이라이트는 된장찌개. 진짜 굿입니다. 버터 간장밥, 골뱅이 국수 등 나중에 더 맛봐야 할 것들은 남겨뒀습니다.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr3_binary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the attn and logits\n",
    "import torch\n",
    "i = 0\n",
    "attns = torch.load(f'outputs/FT3/attentions/batch_{i}.pt')\n",
    "logits = torch.load(f'outputs/FT3/logits/batch_{i}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 12, 184]), torch.Size([32, 2]))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attns.size(), logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_per_token = attns[0][-1]\n",
    "logit = logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0062, 0.0058, 0.0061, 0.0057, 0.0060, 0.0063, 0.0067, 0.0068, 0.0069,\n",
       "        0.0063, 0.0064, 0.0061, 0.0032, 0.0040, 0.0030, 0.0037, 0.0040, 0.0035,\n",
       "        0.0037, 0.0026, 0.0033, 0.0032, 0.0026, 0.0035, 0.0052, 0.0026, 0.0056,\n",
       "        0.0067, 0.0072, 0.0069, 0.0070, 0.0064, 0.0067, 0.0066, 0.0079, 0.0073,\n",
       "        0.0068, 0.0069, 0.0071, 0.0061, 0.0069, 0.0069, 0.0070, 0.0069, 0.0064,\n",
       "        0.0063, 0.0039, 0.0046, 0.0037, 0.0074, 0.0063, 0.0060, 0.0060, 0.0067,\n",
       "        0.0058, 0.0055, 0.0058, 0.0073, 0.0052, 0.0058, 0.0061, 0.0060, 0.0061,\n",
       "        0.0059, 0.0058, 0.0058, 0.0048, 0.0060, 0.0064, 0.0043, 0.0041, 0.0062,\n",
       "        0.0067, 0.0070, 0.0060, 0.0065, 0.0057, 0.0035, 0.0027, 0.0056, 0.0049,\n",
       "        0.0078, 0.0075, 0.0031, 0.0029, 0.0030, 0.0080, 0.0078, 0.0074, 0.0077,\n",
       "        0.0073, 0.0071, 0.0064, 0.0048, 0.0055, 0.0061, 0.0068, 0.0045, 0.0079,\n",
       "        0.0086, 0.0084, 0.0058, 0.0029, 0.0030, 0.0027, 0.0026, 0.0026, 0.0026,\n",
       "        0.0025, 0.0024, 0.0025, 0.0032, 0.0062, 0.0069, 0.0087, 0.0072, 0.0055,\n",
       "        0.0054, 0.0031, 0.0029, 0.0031, 0.0024, 0.0034, 0.0063, 0.0065, 0.0066,\n",
       "        0.0082, 0.0076, 0.0074, 0.0081, 0.0072, 0.0060, 0.0060, 0.0062, 0.0062,\n",
       "        0.0071, 0.0073, 0.0077, 0.0080, 0.0077, 0.0075, 0.0074, 0.0072, 0.0063,\n",
       "        0.0059, 0.0061, 0.0074, 0.0065, 0.0069, 0.0074, 0.0073, 0.0067, 0.0082,\n",
       "        0.0072, 0.0067, 0.0070, 0.0072, 0.0079, 0.0063, 0.0060, 0.0063, 0.0075,\n",
       "        0.0076, 0.0066, 0.0065, 0.0082, 0.0051, 0.0079, 0.0066, 0.0069, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = exclude_cls_sep(attn_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero out the special tokens, [CLS] and [SEP]\n",
    "# Side note: Multi-sentence input may have [SEP] in the middle of the input. Then the code below should change.\n",
    "\n",
    "attn_per_token[0] = 0.0 # idx 0 for [CLS]\n",
    "attn_per_token[torch.nonzero(attn_per_token)[-1]] = 0.0 # last nonzero idx for [SEP]\n",
    "attn_per_token /= attn_per_token.sum() # normalize over the remanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0059, 0.0062, 0.0058, 0.0061, 0.0064, 0.0068, 0.0070, 0.0070,\n",
       "        0.0065, 0.0065, 0.0062, 0.0033, 0.0041, 0.0031, 0.0038, 0.0041, 0.0035,\n",
       "        0.0037, 0.0027, 0.0033, 0.0033, 0.0027, 0.0036, 0.0054, 0.0027, 0.0057,\n",
       "        0.0068, 0.0073, 0.0070, 0.0071, 0.0066, 0.0068, 0.0067, 0.0080, 0.0074,\n",
       "        0.0070, 0.0070, 0.0072, 0.0062, 0.0071, 0.0070, 0.0071, 0.0071, 0.0066,\n",
       "        0.0065, 0.0040, 0.0047, 0.0037, 0.0075, 0.0064, 0.0061, 0.0061, 0.0068,\n",
       "        0.0059, 0.0057, 0.0059, 0.0075, 0.0053, 0.0059, 0.0062, 0.0062, 0.0063,\n",
       "        0.0060, 0.0060, 0.0060, 0.0049, 0.0061, 0.0066, 0.0044, 0.0042, 0.0064,\n",
       "        0.0068, 0.0071, 0.0062, 0.0067, 0.0058, 0.0036, 0.0028, 0.0057, 0.0050,\n",
       "        0.0079, 0.0076, 0.0032, 0.0030, 0.0031, 0.0082, 0.0080, 0.0075, 0.0078,\n",
       "        0.0074, 0.0072, 0.0066, 0.0049, 0.0057, 0.0062, 0.0069, 0.0046, 0.0081,\n",
       "        0.0087, 0.0086, 0.0060, 0.0030, 0.0030, 0.0027, 0.0026, 0.0026, 0.0026,\n",
       "        0.0025, 0.0024, 0.0026, 0.0033, 0.0063, 0.0070, 0.0088, 0.0073, 0.0056,\n",
       "        0.0055, 0.0031, 0.0030, 0.0032, 0.0025, 0.0035, 0.0064, 0.0067, 0.0067,\n",
       "        0.0084, 0.0078, 0.0075, 0.0082, 0.0073, 0.0061, 0.0062, 0.0063, 0.0063,\n",
       "        0.0072, 0.0075, 0.0079, 0.0082, 0.0078, 0.0077, 0.0076, 0.0073, 0.0064,\n",
       "        0.0060, 0.0063, 0.0076, 0.0067, 0.0071, 0.0075, 0.0074, 0.0068, 0.0084,\n",
       "        0.0073, 0.0069, 0.0072, 0.0074, 0.0081, 0.0064, 0.0061, 0.0065, 0.0077,\n",
       "        0.0077, 0.0067, 0.0067, 0.0083, 0.0052, 0.0080, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention score per word, computed by summing scores for tokens in the word\n",
    "words = kr3[0]['Review'].split(' ')\n",
    "attn_per_word = torch.zeros(len(words))\n",
    "tokens_per_word = []\n",
    "\n",
    "# start slicing index by 1\n",
    "# why not 0? Because first token is the [CLS]. cf) We don't have [SEP] in the middle of the input, so we do not consider that.\n",
    "i = 1\n",
    "\n",
    "for j, word in enumerate(words):\n",
    "    # number of tokens in the word\n",
    "    num_tokens = len(tokenizer.tokenize(word))\n",
    "\n",
    "    # slice the attention by num_tokens\n",
    "    tokens_per_word.append(attn_per_token[i:i+num_tokens])\n",
    "    # sum over tokens to obtain attention per word\n",
    "    sum = tokens_per_word[j].sum()\n",
    "    attn_per_word[j] = sum\n",
    "    i += num_tokens\n",
    "\n",
    "assert attn_per_token[i+1] == 0.0 # attn score after all the indexing should be zero (corresponding to [SEP] after normalization)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_attn_per_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/gsdsaml/kr3_rationale/_scratchpad3.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22534e552d47534453227d/home/gsdsaml/kr3_rationale/_scratchpad3.ipynb#ch0000058vscode-remote?line=0'>1</a>\u001b[0m attn_per_word \u001b[39m=\u001b[39m get_attn_per_word(attn)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_attn_per_word' is not defined"
     ]
    }
   ],
   "source": [
    "attn_per_word = get_attn_per_word(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0120) 숙성\n",
      "tensor(0.0182) 돼지고기\n",
      "tensor(0.0397) 전문점입니다.\n",
      "tensor(0.0073) 건물\n",
      "tensor(0.0069) 모양\n",
      "tensor(0.0040) 때문에\n",
      "tensor(0.0072) 매장\n",
      "tensor(0.0092) 모양도\n",
      "tensor(0.0026) 좀\n",
      "tensor(0.0172) 특이하지만\n",
      "tensor(0.0210) 쾌적한\n",
      "tensor(0.0136) 편이고\n",
      "tensor(0.0134) 살짝\n",
      "tensor(0.0223) 레트로\n",
      "tensor(0.0203) 감성으로\n",
      "tensor(0.0210) 분위기\n",
      "tensor(0.0135) 잡아놨습니다.\n",
      "tensor(0.0064) 모든\n",
      "tensor(0.0322) 직원분들께서\n",
      "tensor(0.0128) 전부\n",
      "tensor(0.0173) 가능하다고\n",
      "tensor(0.0127) 멘트\n",
      "tensor(0.0243) 쳐주시며,\n",
      "tensor(0.0119) 고기는\n",
      "tensor(0.0108) 초반\n",
      "tensor(0.0211) 커팅까지는\n",
      "tensor(0.0387) 구워주십니다.\n",
      "tensor(0.0064) 가격\n",
      "tensor(0.0186) 저렴한\n",
      "tensor(0.0075) 편\n",
      "tensor(0.0092) 아니지만\n",
      "tensor(0.0160) 맛은\n",
      "tensor(0.0364) 준수합니다.\n",
      "tensor(0.0281) 등심덧살이\n",
      "tensor(0.0167) 인상\n",
      "tensor(0.0145) 깊었는데\n",
      "tensor(0.0087) 구이로\n",
      "tensor(0.0078) 별로일\n",
      "tensor(0.0025) 줄\n",
      "tensor(0.0082) 알았는데\n",
      "tensor(0.0133) 육향\n",
      "tensor(0.0161) 짙고\n",
      "tensor(0.0110) 얇게\n",
      "tensor(0.0061) 저며\n",
      "tensor(0.0032) 뻑뻑하지\n",
      "tensor(0.0256) 않았습니다.\n",
      "tensor(0.0390) 하이라이트는\n",
      "tensor(0.0319) 된장찌개.\n",
      "tensor(0.0153) 진짜\n",
      "tensor(0.0311) 굿입니다.\n",
      "tensor(0.0136) 버터\n",
      "tensor(0.0264) 간장밥,\n",
      "tensor(0.0219) 골뱅이\n",
      "tensor(0.0151) 국수\n",
      "tensor(0.0073) 등\n",
      "tensor(0.0140) 나중에\n",
      "tensor(0.0073) 더\n",
      "tensor(0.0205) 맛봐야\n",
      "tensor(0.0064) 할\n",
      "tensor(0.0153) 것들은\n",
      "tensor(0.0414) 남겨뒀습니다.\n"
     ]
    }
   ],
   "source": [
    "for attn_, word in zip(attn_per_word, words):\n",
    "    print(attn_, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([60,  2, 46, 26, 32, 18, 47, 49, 33, 51, 45, 22])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = 0.2\n",
    "rationale_idxs = attn_per_word.argsort(descending=True)[:int(len(attn_per_word)*ratio)]\n",
    "rationale_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "rationale_len = len(rationale_idxs)\n",
    "best_contin_score = 0.0\n",
    "\n",
    "for i in range(len(attn_per_word) - rationale_len):\n",
    "    contin_score = attn_per_word[i:i+rationale_len].sum()\n",
    "    if contin_score > best_contin_score:\n",
    "        best_contin_score = contin_score\n",
    "        best_contin_rationale = words[i:i+rationale_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['않았습니다.',\n",
       "  '하이라이트는',\n",
       "  '된장찌개.',\n",
       "  '진짜',\n",
       "  '굿입니다.',\n",
       "  '버터',\n",
       "  '간장밥,',\n",
       "  '골뱅이',\n",
       "  '국수',\n",
       "  '등',\n",
       "  '나중에',\n",
       "  '더'],\n",
       " tensor(0.2483))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_contin_rationale, best_contin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "rationale = [] # words in the rationale, ordered (for sufficiency eval)\n",
    "unrationale = [] # words not in the rationale, ordered (for completeness evla)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    if i in rationale_idxs:\n",
    "        rationale.append(word)\n",
    "    else:\n",
    "        unrationale.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전문점입니다. 직원분들께서 쳐주시며, 구워주십니다. 준수합니다. 등심덧살이 않았습니다. 하이라이트는 된장찌개. 굿입니다. 간장밥, 남겨뒀습니다. \n",
      "\n",
      "숙성 돼지고기 건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편이고 살짝 레트로 감성으로 분위기 잡아놨습니다. 모든 전부 가능하다고 멘트 고기는 초반 커팅까지는 가격 저렴한 편 아니지만 맛은 인상 깊었는데 구이로 별로일 줄 알았는데 육향 짙고 얇게 저며 뻑뻑하지 진짜 버터 골뱅이 국수 등 나중에 더 맛봐야 할 것들은 \n",
      "\n",
      "숙성 돼지고기 전문점입니다. 건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편이고 살짝 레트로 감성으로 분위기 잡아놨습니다. 모든 직원분들께서 전부 가능하다고 멘트 쳐주시며, 고기는 초반 커팅까지는 구워주십니다. 가격 저렴한 편 아니지만 맛은 준수합니다. 등심덧살이 인상 깊었는데 구이로 별로일 줄 알았는데 육향 짙고 얇게 저며 뻑뻑하지 않았습니다. 하이라이트는 된장찌개. 진짜 굿입니다. 버터 간장밥, 골뱅이 국수 등 나중에 더 맛봐야 할 것들은 남겨뒀습니다.\n"
     ]
    }
   ],
   "source": [
    "rationale_text = ' '.join(rationale) # words in the list concated to a sentence\n",
    "unrationale_text = ' '.join(unrationale)\n",
    "\n",
    "print(rationale_text, '\\n')\n",
    "print(unrationale_text, '\\n')\n",
    "print(kr3[0]['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 9665, 25934, 34907, 58303, 48345, 119, 9707, 14279, 37712, 27023, 118683, 12424, 9755, 87281, 21406, 117, 8908, 69592, 16323, 119085, 48345, 119, 9691, 15891, 33188, 48345, 119, 9121, 71013, 118786, 106249, 10739, 9523, 119118, 119081, 48345, 119, 9952, 10739, 17342, 41620, 11018, 9099, 13890, 119245, 21789, 119, 8915, 58303, 48345, 119, 8845, 13890, 118969, 117, 8987, 89045, 118808, 119081, 48345, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([rationale_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensors = {k:torch.tensor(v) for k,v in tokenizer([rationale_text]).items()}\n",
    "output = model(**input_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0015, 0.9985])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(logit, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4205, 0.5795]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(output.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fresh import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5333f1ae2d2dec2c1d8a5fd16a6937e3d6f1660c2626f76e133a4bcfc7001c97"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('cuda11.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
